# Copyright 2019-2020 the ProGraML authors.
#
# Contact Chris Cummins <chrisc.101@gmail.com>.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Node embeddings torch module."""
from typing import Optional, Tuple

import numpy as np
import torch
from torch import nn


class NodeEmbeddings(nn.Module):
    """Construct node embeddings (content embeddings + selector embeddings)
    Args:
    pretrained_embeddings (Tensor, optional) â€“ FloatTensor containing weights for
    the Embedding. First dimension is being passed to Embedding as
    num_embeddings, second as embedding_dim.

    Forward
    Args:
    vocab_ids: <N, 1>
    selector_ids: <N, 1>
    Returns:
    node_states: <N, config.hidden_size>
    """

    # TODO(github.com/ChrisCummins/ProGraML/issues/27):: Maybe LayerNorm and
    # Dropout on node_embeddings?
    #
    # TODO(github.com/ChrisCummins/ProGraML/issues/27):: Make selector embeddings
    # trainable?
    #
    # TODO(github.com/ChrisCummins/ml4pl/issues/12): In the future we may want
    # to be more flexible in supporting multiple types of embeddings tables, but
    # for now I have hardcoded this to always return a tuple
    # <node_embeddings, selector_embeddings>, where inst2vec_embeddings
    # is the augmented table of pre-trained statement embeddings (the
    # augmentation adds !MAGIC, !IMMEDIATE, and !IDENTIFIER vocabulary
    # elements). selector_embeddings is a 2x2 1-hot embedding table:
    # [[1, 0], [0, 1]. The selector_embeddings table is always constant, the
    # inst2vec_embeddings table can be made trainable or re-initialized with
    # random values using the --inst2vec_embeddings flag.

    def __init__(
        self,
        node_embeddings_type: str,
        use_selector_embeddings: bool,
        selector_embedding_value: float = 50,
        embedding_shape: Optional[Tuple[int, int]] = None,
        pretrained_embeddings: Optional[np.array] = None,
    ):
        super().__init__()

        if node_embeddings_type == "pretrained":
            assert embedding_shape is None
            assert pretrained_embeddings is not None
            self._node_embeddings = nn.Embedding.from_pretrained(
                pretrained_embeddings, freeze=True
            )
        elif node_embeddings_type == "finetune":
            assert embedding_shape is None
            assert pretrained_embeddings is not None
            self._node_embeddings = nn.Embedding.from_pretrained(
                pretrained_embeddings, freeze=False
            )
        elif node_embeddings_type == "constant_zero":
            assert embedding_shape is not None
            assert pretrained_embeddings is None
            init = torch.zeros(*embedding_shape)
            self._node_embeddings = nn.Embedding.from_pretrained(init, freeze=True)
        elif node_embeddings_type == "constant_random":
            assert embedding_shape is not None
            assert pretrained_embeddings is None
            init = torch.rand(*embedding_shape)
            self._node_embeddings = nn.Embedding.from_pretrained(init, freeze=True)
        elif node_embeddings_type == "random":
            assert embedding_shape is not None
            assert pretrained_embeddings is None
            self._node_embeddings = nn.Embedding(*embedding_shape)
        else:
            raise ValueError(
                f"Unsupported node embeddings type: {node_embeddings_type}"
            )

        if use_selector_embeddings:
            # The selector embeddings table is a 1-hot encoded table for <true,false>
            # binary selector values.
            selector_init = torch.tensor(
                [[selector_embedding_value, 0], [0, selector_embedding_value]],
                dtype=torch.get_default_dtype(),
            )
            self._selector_embeddings = nn.Embedding.from_pretrained(
                selector_init, freeze=True
            )
        else:
            self._selector_embeddings = None

        self.vocab_size = embedding_shape[0]
        self.text_embedding_dimensionality = embedding_shape[1]
        self.selector_embedding_dimensionality = 2 if self._selector_embeddings else 0
        self.embedding_dimensionality = (
            self.text_embedding_dimensionality + self.selector_embedding_dimensionality
        )

    def forward(self, vocab_ids, selector_ids=None):
        embeddings = self._node_embeddings(vocab_ids)
        if self._selector_embeddings:
            selector_embeddings = self._selector_embeddings(selector_ids)
            embeddings = torch.cat((embeddings, selector_embeddings), dim=1)
        return embeddings
